# -*- coding: utf-8 -*-
"""MentalHealthChatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kxEp0ytI13YjO2eSS0DTbIAAp6jwRDl1

#Preprocessing and Splitting the dataset into Training and Development Datasets
"""
if __name__ == "__main__":
    # 1. Import Libraries
    import pandas as pd
    import re
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import LabelEncoder

    filepath = 'Combined Data.csv'
    df = pd.read_csv(filepath)
    print(df)

    df.status.unique()

    # Encoding Labels
    le = LabelEncoder()
    df['status'] = le.fit_transform(df['status'])

    import re

    def preprocess(sentence):
        # Convert to string and lowercase
        sentence = str(sentence).lower()

        # Remove hashtags
        sentence = re.sub(r'#\S+', '', sentence)

        # Remove mentions
        sentence = re.sub(r'@\S+', '', sentence)

        # Remove punctuation and numbers
        sentence = re.sub(r'[^\w\s]', '', sentence)  # Keeps alphanumeric characters and whitespace
        sentence = re.sub(r'\d+', '', sentence)      # Removes numbers

        # Normalize whitespace
        sentence = ' '.join(sentence.split())

        # Remove URLs
        sentence = re.sub(r'http\S+', '', sentence)

        return sentence

    # Apply preprocessing to the text data
    df['statement'] = df['statement'].map(lambda s: preprocess(s))

    # Shuffle the data and split it into 70% training and 30% development
    df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffling the da

    df.groupby('status').describe()
    df['statement'] = df['statement'].astype(str)

    """process with duplicated data"""

    duplicates_with_status = df[df['statement'].duplicated(keep=False)].sort_values('statement')
    print("Duplicate statements with their status:")
    print(duplicates_with_status[['statement', 'status']])

    """Show the Imbalance of Data"""

    sorted_status_counts = df['status'].value_counts().sort_index()
    print("Sorted unique values and their counts in 'status' column:")
    print(sorted_status_counts)

    """Remove Duplicated Value"""

    df_no_duplicates = df.drop_duplicates(subset=['statement'], keep='first')
    rows_removed = df.shape[0] - df_no_duplicates.shape[0]
    print(f"Number of duplicate rows removed: {rows_removed}")


    df = df_no_duplicates

    sorted_status_counts = df['status'].value_counts().sort_index()
    print("Sorted unique values and their counts in 'status' column:")
    print(sorted_status_counts)

    """Method: resampling"""

    import pandas as pd
    from sklearn.utils import resample

    #set the target num of samples per class
    n_samples = 8000

    df_sampled = pd.DataFrame()

    # Sample Every Category
    for status in df['status'].unique():
        df_class = df[df['status'] == status]

        # up
        if len(df_class) < n_samples:
            df_class_sampled = resample(df_class,
                                        replace=True,      # 有放回采样
                                        n_samples=n_samples,
                                        random_state=42)   # 设置随机种子以确保可重复性
        # down
        elif len(df_class) > n_samples:
            df_class_sampled = resample(df_class,
                                        replace=False,     # 无放回采样
                                        n_samples=n_samples,
                                        random_state=42)   # 设置随机种子以确保可重复性
        else:
            df_class_sampled = df_class

        df_sampled = pd.concat([df_sampled, df_class_sampled])

    # reset index
    df_sampled = df_sampled.reset_index(drop=True)

    print(df_sampled['status'].value_counts())

    print(df_sampled.head())

    print(f"Total number of samples: {len(df_sampled)}")

    # Split into training (70%) and development (30%)
    df_train, dev_df = train_test_split(df_sampled, test_size=0.3, random_state=42)
    df_val, df_test = train_test_split(dev_df, test_size=0.5, random_state=42)

    """If need CSV file, then uncomment the lines below"""

    # Save the training and development datasets into separate CSV files
    # df_train.to_csv('train_data.csv', index=False)
    # dev_df.to_csv('dev_data.csv', index=False)

    """#TRAINING"""

    # df_train = pd.read_csv('train_data.csv')
    # print(df_train)

    # print(df.columns)

    df_train=df_train[['statement', 'status']]
    df_val = df_val[['statement', 'status']]

    df_val.head()

    df_train.status.unique()

    df_train=df_train.sample(frac=1)
    df_train.head(5)

    df_train = df_train[['statement', 'status']]

    df_train.status.value_counts()

    # !pip install simpletransformers
    # !pip install transformers
    #
    # pip install --upgrade simpletransformers

    import pandas as pd
    from simpletransformers.classification import ClassificationModel, ClassificationArgs
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report
    import numpy as np

    np.set_printoptions(threshold=np.inf)

    """#MODEL: BERT"""

    # from simpletransformers.classification import ClassificationModel

    # model = ClassificationModel(
    #     'bert', 'bert-base-uncased',  # Change the pre-trained model to BERT
    #     num_labels=7,                 # Binary classification task
    #     use_cuda=False,               # Disable GPU if needed
    #     args={
    #         "reprocess_input_data": True,          # Reprocess the input data
    #         "use_cached_eval_features": False,     # Don't use cached eval features
    #         "overwrite_output_dir": True,          # Overwrite the output directory
    #         "num_train_epochs": 1                  # Train for 1 epoch
    #     }
    # )

    # model.train_model(df_train)

    """#MODEL: DISTILBERT
    
    """

    model_args = ClassificationArgs(
        reprocess_input_data=True,
        use_cached_eval_features=False,
        overwrite_output_dir=True,
        num_train_epochs=10,
        evaluate_during_training=True,
        evaluate_during_training_steps=1000,
        save_eval_checkpoints=False,
        save_model_every_epoch=False,
    )

    model1=ClassificationModel('distilbert','distilbert-base-cased',num_labels=7,use_cuda=False,args=model_args
    )

    model1.train_model(df_train, eval_df=df_val)
    # Ensure 'statement' column is filled and of string type
    df_test['statement'] = df_test['statement'].fillna("")  # Handle NaN values
    model1.save_model(".")
    # Convert to list for prediction
    statements = df_test['statement'].tolist()

    # Ensure the list is not empty
    if len(statements) == 0:
        print("The test dataset is empty!")
    else:
        # Predict using the model
        predictions1, raw_outputs1 = model1.predict(statements)
        #print(predictions1)  # Display predictions


    import warnings
    warnings.filterwarnings("ignore")
    warnings.simplefilter(action='ignore',category=FutureWarning)
    from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

    print(classification_report(df_test.status,predictions1))

    """#MODEL: MBERT"""

    # model2=ClassificationModel('bert','bert-base-multilingual-cased',num_labels=2,use_cuda=True,args={
    #         "reprocess_input_data" : True,
    #         "use_cached_eval_features":False,
    #         "overwrite_output_dir": True,
    #         "num_train_epochs": 5}
    # )

    # model2.train_model(df_train)

    """#MODEL: ROBERTA"""

    model3 = ClassificationModel(
        "roberta",
        "roberta-base",
        num_labels=2,
        weight=[0,1],
        use_cuda=True,
        args={
            "reprocess_input_data" : True,
            "use_cached_eval_features":False,
            "overwrite_output_dir": True,
            "num_train_epochs": 3 })

    model3.train_model(df_train)



    """#MODEL: LaBSE"""

    # model4=ClassificationModel('bert','setu4993/LaBSE',num_labels=2,use_cuda=False,
    #         args={
    #         "reprocess_input_data" : True,
    #         "use_cached_eval_features":False,
    #         "overwrite_output_dir": True,
    #         "num_train_epochs": 1
    #         })

    # model4.train_model(df_train)

    #  predictions, raw_outputs = model.predict(df_test['statement'].tolist())

    """#DEVELOPMENT
    
    """

    df_test = pd.read_csv('dev_data.csv')
    print(df_test)

    df_test.status.unique()

    df_test=df_test.sample(frac=1)
    df_test.head(5)

    print(df_test['statement'].head())
    print(df_test['statement'].isnull().sum())

    # Ensure 'statement' column is filled and of string type
    df_test['statement'] = df_test['statement'].fillna("")  # Handle NaN values

    # Convert to list for prediction
    statements = df_test['statement'].tolist()

    # Ensure the list is not empty
    if len(statements) == 0:
        print("The test dataset is empty!")
    else:
        # Predict using the model
        predictions1, raw_outputs1 = model1.predict(statements)
        print(predictions1)  # Display predictions

    import warnings
    warnings.filterwarnings("ignore")
    warnings.simplefilter(action='ignore',category=FutureWarning)
    from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

    print(classification_report(df_test.status,predictions1))

    import seaborn as sns
    import matplotlib.pyplot as plt
    from sklearn.metrics import classification_report


    report = classification_report(df_test.status, predictions1, output_dict=True)

    # Convert the report dictionary to a DataFrame for easier visualization
    report_df = pd.DataFrame(report).transpose()

    # Plot the precision, recall, and F1-score as a heatmap
    plt.figure(figsize=(10, 6))
    sns.heatmap(report_df.iloc[:-1, :-1], annot=True, cmap='YlOrBr', fmt='.2f', cbar=True)

    # Customize the plot if needed
    plt.title('Classification Report')
    plt.xlabel('Metrics')
    plt.ylabel('Class Labels')
    plt.tight_layout()
    plt.show()

    from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

    cm1 = confusion_matrix(df_test.status,predictions1)

    cm1

    """#TESTING"""

    # Define your test sentences as separate variables
    test_sentence1 = "I'm feeling very anxious today."
    test_sentence2 = "Despite the chaos around me, I feel at peace."
    test_sentence3 = "I can't help but feel overwhelmed by the weight of my responsibilities."
    test_sentence4 = "Today is a beautiful day, and I'm grateful for everything."
    test_sentence5 = "I'm excited about the future, but I worry about what could go wrong."
    test_sentence6 = "I often feel like I'm in a dark place and can't find a way out."
    test_sentence7 = "I just got a promotion at work, and I'm really proud of myself!"
    test_sentence8 = "I feel numb and detached from everything around me."
    test_sentence9 = "Even though I'm busy, I enjoy the little things in life."
    test_sentence10 = "Sometimes I feel like my anxiety is crippling, but I try to push through."

    # Preprocess the input sentence
    preprocessed_sentence1 = preprocess(test_sentence1)
    preprocessed_sentence2 = preprocess(test_sentence2)
    preprocessed_sentence3 = preprocess(test_sentence3)
    preprocessed_sentence4 = preprocess(test_sentence4)
    preprocessed_sentence5 = preprocess(test_sentence5)
    preprocessed_sentence6 = preprocess(test_sentence6)
    preprocessed_sentence7 = preprocess(test_sentence7)
    preprocessed_sentence8 = preprocess(test_sentence8)
    preprocessed_sentence9 = preprocess(test_sentence9)
    preprocessed_sentence10 = preprocess(test_sentence10)

    # Make the prediction
    predicted_label1, raw_outputs1 = model1.predict([preprocessed_sentence1])  # Wrap the input in a list
    predicted_label2, raw_outputs2 = model1.predict([preprocessed_sentence2])  # Wrap the input in a list
    predicted_label3, raw_outputs3 = model1.predict([preprocessed_sentence3])
    predicted_label4, raw_outputs4 = model1.predict([preprocessed_sentence4])
    predicted_label5, raw_outputs5 = model1.predict([preprocessed_sentence5])
    predicted_label6, raw_outputs6 = model1.predict([preprocessed_sentence6])
    predicted_label7, raw_outputs7 = model1.predict([preprocessed_sentence7])
    predicted_label8, raw_outputs8 = model1.predict([preprocessed_sentence8])
    predicted_label9, raw_outputs9 = model1.predict([preprocessed_sentence9])
    predicted_label10, raw_outputs10 = model1.predict([preprocessed_sentence10])

    # Print the input sentences and their predicted labels
    print("Input Sentence 1:", test_sentence1)
    print("Predicted Numeric Label 1:", predicted_label1)

    print("Input Sentence 2:", test_sentence2)
    print("Predicted Numeric Label 2:", predicted_label2)

    print("Input Sentence 3:", test_sentence3)
    print("Predicted Numeric Label 3:", predicted_label3)

    print("Input Sentence 4:", test_sentence4)
    print("Predicted Numeric Label 4:", predicted_label4)

    print("Input Sentence 5:", test_sentence5)
    print("Predicted Numeric Label 5:", predicted_label5)

    print("Input Sentence 6:", test_sentence6)
    print("Predicted Numeric Label 6:", predicted_label6)

    print("Input Sentence 7:", test_sentence7)
    print("Predicted Numeric Label 7:", predicted_label7)

    print("Input Sentence 8:", test_sentence8)
    print("Predicted Numeric Label 8:", predicted_label8)

    print("Input Sentence 9:", test_sentence9)
    print("Predicted Numeric Label 9:", predicted_label9)

    print("Input Sentence 10:", test_sentence10)
    print("Predicted Numeric Label 10:", predicted_label10)

    # Step 5: Convert predicted labels back to original status for each prediction
    original_label1 = le.inverse_transform(predicted_label1)
    original_label2 = le.inverse_transform(predicted_label2)
    original_label3 = le.inverse_transform(predicted_label3)
    original_label4 = le.inverse_transform(predicted_label4)
    original_label5 = le.inverse_transform(predicted_label5)
    original_label6 = le.inverse_transform(predicted_label6)
    original_label7 = le.inverse_transform(predicted_label7)
    original_label8 = le.inverse_transform(predicted_label8)
    original_label9 = le.inverse_transform(predicted_label9)
    original_label10 = le.inverse_transform(predicted_label10)

    # Print the original labels along with raw outputs for each sentence
    print("Input Sentence 1:", test_sentence1)
    print("Predicted Numeric Label 1:", predicted_label1)
    print("Original Predicted Label 1:", original_label1[0])  # Display the original status label
    print("Raw Outputs 1:", raw_outputs1)  # Display raw outputs (if needed)

    print("Input Sentence 2:", test_sentence2)
    print("Predicted Numeric Label 2:", predicted_label2)
    print("Original Predicted Label 2:", original_label2[0])  # Display the original status label
    print("Raw Outputs 2:", raw_outputs2)  # Display raw outputs (if needed)

    print("Input Sentence 3:", test_sentence3)
    print("Predicted Numeric Label 3:", predicted_label3)
    print("Original Predicted Label 3:", original_label3[0])  # Display the original status label
    print("Raw Outputs 3:", raw_outputs3)  # Display raw outputs (if needed)

    print("Input Sentence 4:", test_sentence4)
    print("Predicted Numeric Label 4:", predicted_label4)
    print("Original Predicted Label 4:", original_label4[0])  # Display the original status label
    print("Raw Outputs 4:", raw_outputs4)  # Display raw outputs (if needed)

    print("Input Sentence 5:", test_sentence5)
    print("Predicted Numeric Label 5:", predicted_label5)
    print("Original Predicted Label 5:", original_label5[0])  # Display the original status label
    print("Raw Outputs 5:", raw_outputs5)  # Display raw outputs (if needed)

    print("Input Sentence 6:", test_sentence6)
    print("Predicted Numeric Label 6:", predicted_label6)
    print("Original Predicted Label 6:", original_label6[0])  # Display the original status label
    print("Raw Outputs 6:", raw_outputs6)  # Display raw outputs (if needed)

    print("Input Sentence 7:", test_sentence7)
    print("Predicted Numeric Label 7:", predicted_label7)
    print("Original Predicted Label 7:", original_label7[0])  # Display the original status label
    print("Raw Outputs 7:", raw_outputs7)  # Display raw outputs (if needed)

    print("Input Sentence 8:", test_sentence8)
    print("Predicted Numeric Label 8:", predicted_label8)
    print("Original Predicted Label 8:", original_label8[0])  # Display the original status label
    print("Raw Outputs 8:", raw_outputs8)  # Display raw outputs (if needed)

    print("Input Sentence 9:", test_sentence9)
    print("Predicted Numeric Label 9:", predicted_label9)
    print("Original Predicted Label 9:", original_label9[0])  # Display the original status label
    print("Raw Outputs 9:", raw_outputs9)  # Display raw outputs (if needed)

    print("Input Sentence 10:", test_sentence10)
    print("Predicted Numeric Label 10:", predicted_label10)
    print("Original Predicted Label 10:", original_label10[0])  # Display the original status label
    print("Raw Outputs 10:", raw_outputs10)  # Display raw outputs (if needed)

    """#OTHER MODELS
    
    # XLM Roberta
    """

    # model5 = ClassificationModel(
    #     "xlmroberta",
    #     "xlm-roberta-base",
    #     num_labels=4,
    #     weight=[0,1,2,3],
    #     use_cuda=True,
    #     args={
    #         "reprocess_input_data" : True,
    #         "use_cached_eval_features":False,
    #         "overwrite_output_dir": True,
    #         "num_train_epochs": 3 })

    # model5.train_model(df1_train)

    # predictions5, raw_outputs5 = model5.predict(df_test['Text'].tolist())

    # print(classification_report(df_test.Label,predictions5))

    """# INDIC BERT"""

    # model6 = ClassificationModel("albert","ai4bharat/indic-bert",num_labels=4,use_cuda=True,args={
    #         "reprocess_input_data" : True,
    #         "use_cached_eval_features":False,
    #         "overwrite_output_dir": True,
    #         "num_train_epochs": 5})

    # model6.train_model(df1_train)

    # predictions6, raw_outputs6 = model6.predict(df_test['Text'].tolist())

    # print(classification_report(df_test.Label,predictions6))

    """# Muril"""

    # model7 = ClassificationModel("bert","google/muril-base-cased" ,num_labels=4,use_cuda=True,args={
    #         "reprocess_input_data" : True,
    #         "use_cached_eval_features":False,
    #         "overwrite_output_dir": True,
    #         "num_train_epochs": 5})

    # model7.train_model(df1_train)

    # predictions7, raw_outputs7 = model7.predict(df_test['Text'].tolist())

    # print(classification_report(df_test.Label,predictions7))